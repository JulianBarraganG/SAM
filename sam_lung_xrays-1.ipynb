{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f400486b",
   "metadata": {
    "id": "f400486b"
   },
   "source": [
    "# Segmenting Lung X-ray Images with the Segment Anything Model\n",
    "### Advanced Deep Learning 2022\n",
    "Notebook written by [Jakob Ambsdorf](mailto:jaam@di.ku.dk).\n",
    "Lung x-ray code originally written by Mathias Perslev. It has been changed slightly by Christian Igel and subsequently slightly updated [Stefan Sommer](mailto:sommer@di.ku.dk). Finaly changes have been made by [Julian Barragan](mailto:xjulianbarragan@gmail.com) for an assignment task in ADL 2024.\n",
    "SAM related code (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "We consider the data described in:\n",
    "Bram van Ginneken, Mikkel B. Stegmann, Marco Loog. [Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database](https://doi.org/10.1016/j.media.2005.02.002). *Medical Image Analysis* 10(1): 19-40, 2006\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1ae39ff",
   "metadata": {
    "id": "a1ae39ff"
   },
   "source": [
    "## Object masks from prompts with SAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a4b25c",
   "metadata": {
    "id": "b4a4b25c"
   },
   "source": [
    "The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt. \n",
    "\n",
    "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "644532a8",
   "metadata": {
    "id": "644532a8"
   },
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07fabfee",
   "metadata": {
    "id": "07fabfee"
   },
   "source": [
    "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea65efc",
   "metadata": {
    "id": "5ea65efc"
   },
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd9a89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91dd9a89",
    "outputId": "fa11e5ff-9608-4771-d082-b05c8acfea8e"
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "    \n",
    "    !mkdir images\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
    "        \n",
    "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be845da",
   "metadata": {
    "id": "0be845da"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33681dd1",
   "metadata": {
    "id": "33681dd1"
   },
   "source": [
    "Necessary imports and helper functions for displaying points, boxes, and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b28288",
   "metadata": {
    "id": "69b28288"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc90d5",
   "metadata": {
    "id": "29bc90d5"
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax, color='green'):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=color, facecolor=(0,0,0,0), lw=2))    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06ca884f",
   "metadata": {},
   "source": [
    "# Download model checkpoint\n",
    "The checkpoint is 2.39GB, takes a few minutes for most bandwidths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7410dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "filename = \"sam_vit_h_4b8939.pth\"\n",
    "folder = \"models\"\n",
    "\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "filepath = os.path.join(folder, filename)\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    # Get the file size before downloading\n",
    "    file_size = int(urllib.request.urlopen(url).info().get(\"Content-Length\", -1))\n",
    "\n",
    "    # Start the download with progress bar\n",
    "    with tqdm(unit=\"B\", unit_scale=True, unit_divisor=1024, total=file_size, desc=filename, ncols=80) as pbar:\n",
    "        urllib.request.urlretrieve(url, filepath, reporthook=lambda b, bsize, t: pbar.update(bsize))\n",
    "else:\n",
    "    print(\"Checkpoint file already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"models/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "VkTX9wam7nhj",
   "metadata": {
    "id": "VkTX9wam7nhj"
   },
   "source": [
    "# Chest X-ray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l0RJqwoq8kHm",
   "metadata": {
    "id": "l0RJqwoq8kHm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LhXWkNR47rxA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhXWkNR47rxA",
    "outputId": "cf3e397a-c746-4f91-d183-21432580e718"
   },
   "outputs": [],
   "source": [
    "# Mount Google drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive/')\n",
    "    os.chdir('gdrive/MyDrive/ADL2022')\n",
    "except:\n",
    "    print('Google drive not mounted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are getting a download error, comment in the following lines:\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ET9ewnZ_8N7t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ET9ewnZ_8N7t",
    "outputId": "e44d152c-0475-408f-8f44-ed31d6609ad5"
   },
   "outputs": [],
   "source": [
    "# Load database with chest X-rays with lung segmentations.\n",
    "data_root='./datasets'\n",
    "data_npz='lung_field_dataset.npz'\n",
    "data_fn = os.path.join(data_root, \"lung_field_dataset.npz\")\n",
    "force_download = False\n",
    "\n",
    "if (not os.path.exists(data_fn)) or force_download:\n",
    "    download_url(\"https://sid.erda.dk/share_redirect/gCTc6o3KAh\", data_root, data_npz)\n",
    "else:\n",
    "    print('Using existing', data_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MCnFSLuY5cMZ",
   "metadata": {
    "id": "MCnFSLuY5cMZ"
   },
   "outputs": [],
   "source": [
    "def plot_image_with_segmentation(image, segmentation, ax=None):\n",
    "    \"\"\"\n",
    "    Plots an image with overlayed segmentation mask\n",
    "    \n",
    "    Returns: plt.fig and ax objects\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "    mask = np.ma.masked_where(segmentation == 0, segmentation)\n",
    "    ax.imshow(mask.squeeze(), cmap=\"Set1\", alpha=0.5)\n",
    "    return plt.gcf(), ax\n",
    "\n",
    "\n",
    "def load_npz_dataset(path, keys=('x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test')):\n",
    "    archive = np.load(path)\n",
    "    return [archive.get(key) for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_interval(image, from_min, from_max, to_min, to_max):\n",
    "    \"\"\"\n",
    "    Map values from [from_min, from_max] to [to_min, to_max]\n",
    "    \"\"\"\n",
    "    from_range = from_max - from_min\n",
    "    to_range = to_max - to_min\n",
    "    # scaled = np.array((image - from_min) / float(from_range), dtype=float)\n",
    "    scaled = (image - from_min) / float(from_range)\n",
    "    return to_min + (scaled * to_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5737be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "    The boxes are expected to be in xyxy format (x_min, y_min, x_max, y_max).\n",
    "\n",
    "    Args:\n",
    "    - box1, box2: Tensors of shape (4,) representing the coordinates of the boxes.\n",
    "                  box1 should be of type uint8 and box2 should be of type float32.\n",
    "\n",
    "    Returns:\n",
    "    - iou: Float, the Intersection over Union of the two boxes.\n",
    "    \"\"\"\n",
    "    # Ensure the boxes are of the same type\n",
    "    box1 = box1.float()\n",
    "    box2 = box2.float()\n",
    "\n",
    "    # Calculate the coordinates of the intersection box\n",
    "    x1 = torch.max(box1[0], box2[0])\n",
    "    y1 = torch.max(box1[1], box2[1])\n",
    "    x2 = torch.min(box1[2], box2[2])\n",
    "    y2 = torch.min(box1[3], box2[3])\n",
    "\n",
    "    # Calculate the area of the intersection box\n",
    "    inter_area = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "\n",
    "    # Calculate the area of both bounding boxes\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # Calculate the area of the union\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    # Compute the IoU\n",
    "    iou = inter_area / union_area\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aI_AlP5V5p8-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "aI_AlP5V5p8-",
    "outputId": "67d9db74-985e-4268-fd43-68d9f3b40ae7"
   },
   "outputs": [],
   "source": [
    "# Load train/val/test data (0 for original)\n",
    "x0_train, y0_train, x0_val, y0_val, x0_test, y0_test = load_npz_dataset(data_fn)\n",
    "\n",
    "# TODO: \n",
    "# Bring images into the correct format for SAM:\n",
    "# Image shape: (N, H, W, C=3)\n",
    "# Mask shape: (N, H, W)\n",
    "# Values: [0, 255] (uint8)\n",
    "\n",
    "def data_for_sam(x, y):\n",
    "    \"\"\"\n",
    "    Rescale images and masks for SAM\n",
    "    Makes grayscale images into RGB images /w uint8 values 0 to 255\n",
    "    \"\"\"\n",
    "    new_x = []\n",
    "    for img in x:\n",
    "        img_mapped = map_interval(img, img.min(), img.max(), 0, 255)\n",
    "        img_copied = img_mapped.repeat(3, axis=-1)\n",
    "        img_new = img_copied.astype(np.uint8)\n",
    "        new_x.append(img_new)\n",
    "        \n",
    "    return np.array(new_x), y.reshape(y.shape[0], y.shape[1], y.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52397af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = data_for_sam(x0_train, y0_train)\n",
    "x_val, y_val = data_for_sam(x0_val, y0_val)\n",
    "x_test, y_test = data_for_sam(x0_test, y0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your data should pass the following asserts\n",
    "assert x_train.shape == (112, 256, 256, 3)\n",
    "assert y_train.shape == (112, 256, 256)\n",
    "assert x_val.shape == (12, 256, 256, 3)\n",
    "assert y_val.shape == (12, 256, 256)\n",
    "assert x_test.shape == (123, 256, 256, 3)\n",
    "assert y_test.shape == (123, 256, 256)\n",
    "\n",
    "assert x_train.dtype == y_train.dtype == np.uint8\n",
    "assert np.min(x_train) == 0\n",
    "assert np.max(x_train) == 255\n",
    "example_mask = y_train[0].copy()\n",
    "# Plot an example\n",
    "fig, ax = plot_image_with_segmentation(x_train[0], example_mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9yHsXs2QugdE",
   "metadata": {
    "id": "9yHsXs2QugdE"
   },
   "source": [
    "# Single Example image\n",
    "\n",
    "Let's try to run SAM on a single example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Center of Mass (COM) for each mask, and subsequently the COM of all COMs\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "def calculate_coms_for_sides(masks):\n",
    "    left_coms = []\n",
    "    right_coms = []\n",
    "    center_split = 128  # Split masks in the middle\n",
    "    for mask in masks:\n",
    "        left_mask, right_mask = mask.copy(), mask.copy()\n",
    "        left_mask[:, center_split:] = 0 # Left side mask (x from 0 to 127)\n",
    "        right_mask[:, :center_split] = 0 # Right side mask (x from 128 to 255)\n",
    "\n",
    "        # Calculate COM for each side if there are any positive pixels\n",
    "        if np.any(left_mask):\n",
    "            left_com = center_of_mass(left_mask)\n",
    "            # Adjust x-coordinate for left side to fit the original mask coordinates\n",
    "            left_com = (left_com[0], left_com[1])\n",
    "            left_coms.append(left_com)\n",
    "        \n",
    "        if np.any(right_mask):\n",
    "            right_com = center_of_mass(right_mask)\n",
    "            # Adjust x-coordinate for right side to fit the original mask coordinates\n",
    "            right_com = (right_com[0], right_com[1])\n",
    "            right_coms.append(right_com)\n",
    "    \n",
    "    return np.array(left_coms), np.array(right_coms)\n",
    "\n",
    "def calculate_mean_com(coms):\n",
    "    \"\"\"Returns mean as an integer\"\"\"\n",
    "    mean = np.mean(coms, axis=0)\n",
    "    rounded = np.round(mean).astype(int)\n",
    "    xy_correction = np.array([rounded[1], rounded[0]])\n",
    "    return xy_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_lung_coms, right_lung_coms = calculate_coms_for_sides(y_train)\n",
    "left_lung_mean_com = calculate_mean_com(left_lung_coms)\n",
    "right_lung_mean_com = calculate_mean_com(right_lung_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m6tcM2rQs_Hj",
   "metadata": {
    "id": "m6tcM2rQs_Hj"
   },
   "outputs": [],
   "source": [
    "example_img, example_mask = x_train[0], y_train[0]\n",
    "\n",
    "input_points = np.array([left_lung_mean_com, right_lung_mean_com, [128, 128], [128, 240], [15, 50], [256-15, 50]]) # Picked left and right lung COM, center, top, and two points close to shoulders\n",
    "input_label = np.array([1, 1, 0, 0, 0, 0]) # Get labels for the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BhM_AohdukYD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "id": "BhM_AohdukYD",
    "outputId": "60ba6cc0-1675-4266-9b2e-a893039509ac"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(example_img)\n",
    "show_points(input_points, input_label, plt.gca()) # You may also use other prompt methods!\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PS-UajooqKDN",
   "metadata": {
    "id": "PS-UajooqKDN"
   },
   "outputs": [],
   "source": [
    "predictor.set_image(example_img)\n",
    "\n",
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_points,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iE0pIkugqV-i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iE0pIkugqV-i",
    "outputId": "94e9aef4-e541-4017-a3f8-075a8c6c7023"
   },
   "outputs": [],
   "source": [
    "# Show the predicted masks for single image example of engineered click prompts\n",
    "num_masks = len(masks)\n",
    "\n",
    "fig, axes = plt.subplots(1, num_masks, figsize=(6 * num_masks, 6))\n",
    "\n",
    "# Iterate through each mask and corresponding score\n",
    "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    axes[i].imshow(example_img.squeeze())\n",
    "    show_mask(mask, axes[i])\n",
    "    show_points(input_points, input_label, axes[i])\n",
    "    axes[i].set_title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "    axes[i].axis('off')\n",
    "fig.text(0.5, 0.04, 'Predicted masks for engineered click prompts', ha='center', fontsize=22)\n",
    "# Make directory for plots if it doesn't exist\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "plt.savefig(\"plots/single_img_masks.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e2bf35b",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea9a1e",
   "metadata": {},
   "source": [
    "## Internal Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "f1 = torchmetrics.F1Score(task=\"binary\")\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for img, mask_gt in zip(x_val, y_val):\n",
    "    predictor.set_image(img)\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_points,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    mask_pred = masks[np.argmax(scores)] # Get the mask with the highest score according to SAM\n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    mask_pred_tensor = torch.tensor(mask_pred, dtype=torch.int)\n",
    "    mask_gt_tensor = torch.tensor(mask_gt, dtype=torch.int)\n",
    "    \n",
    "    # Calculate the F1 score for the current prediction\n",
    "    current_f1 = f1(mask_pred_tensor, mask_gt_tensor).item()\n",
    "    f1_scores.append(current_f1)\n",
    "    \n",
    "f1_scores = np.array(f1_scores)\n",
    "mean_f1 = f1_scores.mean() # TODO: Compute mean F1 score\n",
    "std_f1 = f1_scores.std() # TODO: Compute standard deviation of F1 scores\n",
    "\n",
    "print(f\"Mean F1 score: {mean_f1:.4f}\")\n",
    "print(f\"Standard deviation: {std_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27d28b",
   "metadata": {},
   "source": [
    "## Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "f1 = torchmetrics.F1Score(task=\"binary\")\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for img, mask_gt in zip(x_test, y_test):\n",
    "    predictor.set_image(img)\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_points,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    mask_pred = masks[1] # Use the second mask heuristically as the prediction\n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    mask_pred_tensor = torch.tensor(mask_pred, dtype=torch.int)\n",
    "    mask_gt_tensor = torch.tensor(mask_gt, dtype=torch.int)\n",
    "    \n",
    "    # Calculate the F1 score for the current prediction\n",
    "    current_f1 = f1(mask_pred_tensor, mask_gt_tensor).item()\n",
    "    f1_scores.append(current_f1)\n",
    "    \n",
    "f1_scores = np.array(f1_scores)\n",
    "mean_f1 = f1_scores.mean() # TODO: Compute mean F1 score\n",
    "std_f1 = f1_scores.std() # TODO: Compute standard deviation of F1 scores\n",
    "\n",
    "print(f\"Mean F1 score: {mean_f1:.4f}\")\n",
    "print(f\"Standard deviation: {std_f1:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b6b9a84",
   "metadata": {},
   "source": [
    "# Using Bounding Boxes from GT segmentations as Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25701a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding boxes from segmentation masks\n",
    "# bonding box format [x0, y0, x1, y1]\n",
    "\n",
    "# TODO: Implement bounding box extraction from segmentation masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130ffdf",
   "metadata": {},
   "source": [
    "## Single Image Bounding Box example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bounding box from ground truth mask\n",
    "def box_from_gt(mask):\n",
    "    \"\"\"\n",
    "    Intended input: left gt and right gt\n",
    "    Returns: list bounding box\n",
    "    \"\"\"\n",
    "    y, x = np.where(mask)\n",
    "    x0, x1 = x.min(), x.max()\n",
    "    y0, y1 = y.min(), y.max()\n",
    "    return [x0, y0, x1, y1]\n",
    "\n",
    "def calculate_bounding_box(mask):\n",
    "    \"\"\"\n",
    "    Calculate bounding box from mask\n",
    "    \"\"\"\n",
    "    left_gt, right_gt = mask.copy(), mask.copy()\n",
    "    left_gt[:, 128:] = 0\n",
    "    right_gt[:, :128] = 0\n",
    "    return box_from_gt(left_gt), box_from_gt(right_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = torch.tensor(calculate_bounding_box(example_mask), device=predictor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(example_img)\n",
    "transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, example_img.shape[:2])\n",
    "\n",
    "# This is a different predict method which takes transformed inputs\n",
    "masks, scores, logits = predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    multimask_output=False,\n",
    "    boxes=transformed_boxes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39021556",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(example_img)\n",
    "for box in input_boxes:\n",
    "    show_box(box.cpu().numpy(), plt.gca())\n",
    "for mask in masks:\n",
    "    show_mask(mask.detach().cpu().numpy(), plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99551a54",
   "metadata": {},
   "source": [
    "## train, val and test boxes from GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89178dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two bounding boxes for each image from the ground-truth lung segmentation masks for the train, val and test splits\n",
    "train_boxes = [torch.tensor(calculate_bounding_box(mask), device=predictor.device) for mask in y_train]\n",
    "val_boxes = [torch.tensor(calculate_bounding_box(mask), device=predictor.device) for mask in y_val]\n",
    "test_boxes = [torch.tensor(calculate_bounding_box(mask), device=predictor.device) for mask in y_test]\n",
    "\n",
    "# Transform bounding boxes to the format expected by the SAM model\n",
    "train_boxes_transformed = [predictor.transform.apply_boxes_torch(box, (256, 256)) for i, box in enumerate(train_boxes)]\n",
    "val_boxes_transformed = [predictor.transform.apply_boxes_torch(box, (256, 256)) for i, box in enumerate(val_boxes)]\n",
    "test_boxes_transformed = [predictor.transform.apply_boxes_torch(box, (256, 256)) for i, box in enumerate(test_boxes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the bounding box extraction on the validation set\n",
    "f1_scores = []\n",
    "\n",
    "f1_box = torchmetrics.F1Score(task=\"binary\")\n",
    "\n",
    "for i, (img, mask_gt) in enumerate(zip(x_val, y_val)):\n",
    "    predictor.set_image(img)\n",
    "    masks, score, logits = predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=val_boxes_transformed[i],\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    # Combine the two masks from the two bounding boxes\n",
    "    mask_combined = (masks[0] + masks[1])[0].detach().cpu().numpy()\n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    mask_pred_tensor = torch.tensor(mask_combined, dtype=torch.int)\n",
    "    mask_gt_tensor = torch.tensor(mask_gt, dtype=torch.int)\n",
    "    \n",
    "    # Calculate the F1 score for the current prediction\n",
    "    current_f1 = f1_box(mask_pred_tensor, mask_gt_tensor).item()\n",
    "    f1_scores.append(current_f1)\n",
    "    \n",
    "f1_scores = np.array(f1_scores)\n",
    "mean_f1 = f1_scores.mean() # TODO: Compute mean F1 score\n",
    "std_f1 = f1_scores.std() # TODO: Compute standard deviation of F1 scores\n",
    "\n",
    "print(f\"Mean F1 score: {mean_f1:.4f}\")\n",
    "print(f\"Standard deviation: {std_f1:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e28ad18",
   "metadata": {},
   "source": [
    "# Object Detection Model to predict Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement an object detection model to find the left and right lung bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3588cb6",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def convert_bbox_to_yolo_format(box, img_width, img_height):\n",
    "    \"\"\"Convert bounding box format from [x_min, y_min, x_max, y_max] \n",
    "    to string '<classid> <x_center> <y_center> <width> <height>.\"\"\"\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    x_center = (x_min + x_max) / 2.0 / img_width\n",
    "    y_center = (y_min + y_max) / 2.0 / img_height\n",
    "    width = (x_max - x_min) / img_width\n",
    "    height = (y_max - y_min) / img_height\n",
    "    x_center, y_center, width, height = x_center.item(), y_center.item(), width.item(), height.item()\n",
    "    return f\"{x_center} {y_center} {width} {height}\\n\"\n",
    "\n",
    "def save_yolo_labels(path, images, bboxes):\n",
    "    \"\"\"Save YOLO labels for each image in the dataset.\"\"\"\n",
    "    os.makedirs(path + '/images', exist_ok=True)\n",
    "    os.makedirs(path + '/labels', exist_ok=True)\n",
    "    for i, image in enumerate(images):\n",
    "        img_path = path + f'/images/image_{i+1}.jpg'\n",
    "        cv2.imwrite(img_path, image)\n",
    "\n",
    "        label_path = path + f'/labels/image_{i+1}.txt'\n",
    "        with open(label_path, 'w') as f:\n",
    "            for j, box in enumerate(bboxes[i]):\n",
    "                class_id = j  # 0 for left lung, 1 for right lung\n",
    "                yolo_format_box = convert_bbox_to_yolo_format(box, 256, 256)\n",
    "                f.write(f\"{class_id} {yolo_format_box}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04119e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'datasets/yolo'\n",
    "save_yolo_labels(file_path + '/x_train', x_train, train_boxes)\n",
    "save_yolo_labels(file_path + '/x_val', x_val, val_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "data = {\n",
    "    'train': 'C:/Users/hulig/OneDrive - University of Copenhagen/ML/ADL/A4/datasets/yolo/x_train/images',\n",
    "    'val': 'C:/Users/hulig/OneDrive - University of Copenhagen/ML/ADL/A4/datasets/yolo/x_val/images',\n",
    "    'nc': 2,\n",
    "    'names': ['left_lung', 'right_lung']\n",
    "}\n",
    "\n",
    "with open('datasets/yolo/dataset.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"models/yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd datasets/yolo\n",
    "\n",
    "# model.train(data='dataset.yaml', epochs=10, imgsz=256, batch=4, device=device)\n",
    "\n",
    "# %cd C:/Users/hulig/OneDrive - University of Copenhagen/ML/ADL/A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fcd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_yolo_labels(file_path + '/x_test', x_test, test_boxes)\n",
    "\n",
    "# Load the trained model (make sure to replace the path with the correct path to the model)\n",
    "model = YOLO(\"datasets/yolo/runs/detect/train52/weights/best.pt\")\n",
    "\n",
    "# Get the predicted bounding boxes\n",
    "test_filepath = 'datasets/yolo/x_test/images'\n",
    "predictions = model.predict(test_filepath, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd567c4c",
   "metadata": {},
   "source": [
    "# Plot example GT Bbox with Predicted BBox on Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c514a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select three images, their ground truth boxes, and predictions\n",
    "from matplotlib.lines import Line2D\n",
    "indices = [0, 1, 2]\n",
    "images = [x_test[i] for i in indices]\n",
    "gt_boxes = [test_boxes[i].detach().cpu().numpy() for i in indices]\n",
    "pred_boxes = [predictions[i].boxes.xyxy.detach().cpu().numpy() for i in indices]\n",
    "\n",
    "# Create the plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for ax, img, gt_box, pred_box in zip(axes, images, gt_boxes, pred_boxes):\n",
    "    ax.imshow(img)\n",
    "    for box in gt_box:\n",
    "        show_box(box, ax)\n",
    "    for box in pred_box:\n",
    "        show_box(box, ax, color='red')\n",
    "    ax.axis('on')\n",
    "\n",
    "# Create custom legend handles\n",
    "legend_elements = [Line2D([0], [0], color='green', lw=2, label='Ground truth'),\n",
    "                   Line2D([0], [0], color='red', lw=2, label='Predicted')]\n",
    "\n",
    "# Add legend outside the last subplot\n",
    "axes[-1].legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feacb18d",
   "metadata": {},
   "source": [
    "## Calculate IoU on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "all_ious = []\n",
    "for i in range(len(predictions)):\n",
    "    pred_boxes = predictions[i].boxes.xyxy.detach().cpu().numpy()\n",
    "    iou = box_iou(torch.tensor(pred_boxes), torch.tensor(test_boxes[i], device='cpu'))\n",
    "    # assert that iou is a diagonal matrix\n",
    "    if (iou[0, 1] != 0 and iou[1, 0] != 0):\n",
    "        if (iou[0, 0] != 0 and iou[1, 1] != 0):\n",
    "            print(f\"IOU matrix is not diagonal for image {i}\")\n",
    "            print(iou)\n",
    "        break\n",
    "    all_ious.append(iou.sum())\n",
    "    \n",
    "mean_iou = np.mean(np.array(all_ious))/2\n",
    "print(f\"Mean IoU score for predicted images: {mean_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lowest and highest IoU scores\n",
    "min_iou, max_iou = min(all_ious) / 2, max(all_ious) / 2\n",
    "\n",
    "# Get the index of the image with the lowest and highest IoU score\n",
    "min_iou_index, max_iou_index = all_ious.index(min(all_ious)),  all_ious.index(max(all_ious))\n",
    "\n",
    "# Select three images, their ground truth boxes, and predictions\n",
    "from matplotlib.lines import Line2D\n",
    "indices = [min_iou_index, max_iou_index]\n",
    "images = [x_test[i] for i in indices]\n",
    "gt_boxes = [test_boxes[i].detach().cpu().numpy() for i in indices]\n",
    "pred_boxes = [predictions[i].boxes.xyxy.detach().cpu().numpy() for i in indices]\n",
    "\n",
    "# Create the plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "titles = [\"Min IoU score for predicted image (test)\", \"Max IoU score for predicted image (test)\"]\n",
    "\n",
    "for ax, img, gt_box, pred_box, title in zip(axes, images, gt_boxes, pred_boxes, titles):\n",
    "    ax.imshow(img)\n",
    "    for box in gt_box:\n",
    "        show_box(box, ax)\n",
    "    for box in pred_box:\n",
    "        show_box(box, ax, color='red')\n",
    "    ax.axis('on')\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Create custom legend handles\n",
    "legend_elements = [Line2D([0], [0], color='green', lw=2, label='Ground truth'),\n",
    "                   Line2D([0], [0], color='red', lw=2, label='Predicted')]\n",
    "\n",
    "# Add legend outside the last subplot\n",
    "axes[0].legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Create an additional legend for IoU scores\n",
    "iou_legend_elements = [Line2D([0], [0], color='white', label=f'Min IoU: {min_iou:.4f}'),\n",
    "                       Line2D([0], [0], color='white', label=f'Max IoU: {max_iou:.4f}')]\n",
    "\n",
    "# Add the additional IoU legend outside the first subplot\n",
    "axes[-1].legend(handles=iou_legend_elements, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d07ab",
   "metadata": {},
   "source": [
    "# Prompting SAM with Predicted BBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8546a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform predicted bboxes to the format expected by the SAM model\n",
    "list_of_preds = [pred.boxes.xyxy for pred in predictions]\n",
    "\n",
    "preds_transformed = [predictor.transform.apply_boxes_torch(box, (256, 256)) for box in list_of_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the bounding box extraction on the validation set\n",
    "f1_scores = []\n",
    "\n",
    "f1_box = torchmetrics.F1Score(task=\"binary\")\n",
    "\n",
    "for i, (img, mask_gt) in enumerate(zip(x_test, y_test)):\n",
    "    predictor.set_image(img)\n",
    "    masks, score, logits = predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=preds_transformed[i],\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    # Combine the two masks from the two bounding boxes\n",
    "    mask_combined = (masks[0] + masks[1])[0].detach().cpu().numpy()\n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    mask_pred_tensor = torch.tensor(mask_combined, dtype=torch.int)\n",
    "    mask_gt_tensor = torch.tensor(mask_gt, dtype=torch.int)\n",
    "    \n",
    "    # Calculate the F1 score for the current prediction\n",
    "    current_f1 = f1_box(mask_pred_tensor, mask_gt_tensor).item()\n",
    "    f1_scores.append(current_f1)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"At iteration {i} with current F1 score: {current_f1:.4f}\")\n",
    "    \n",
    "f1_scores = np.array(f1_scores)\n",
    "mean_f1 = f1_scores.mean() # TODO: Compute mean F1 score\n",
    "std_f1 = f1_scores.std() # TODO: Compute standard deviation of F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cbe61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print standalone for good measure\n",
    "print(f\"Mean F1 score: {mean_f1:.4f}\")\n",
    "print(f\"Standard deviation: {std_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
