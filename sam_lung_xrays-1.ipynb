{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f400486b",
   "metadata": {
    "id": "f400486b"
   },
   "source": [
    "# Segmenting Lung X-ray Images with the Segment Anything Model\n",
    "### Advanced Deep Learning 2022\n",
    "Notebook written by [Jakob Ambsdorf](mailto:jaam@di.ku.dk).\n",
    "Lung x-ray code originally written by Mathias Perslev. It has been changed slightly by Christian Igel and subsequently slightly updated [Stefan Sommer](mailto:sommer@di.ku.dk). Finaly changes have been made by [Julian Barragan](mailto:xjulianbarragan@gmail.com) for an assignment task in ADL 2024.\n",
    "SAM related code (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "We consider the data described in:\n",
    "Bram van Ginneken, Mikkel B. Stegmann, Marco Loog. [Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database](https://doi.org/10.1016/j.media.2005.02.002). *Medical Image Analysis* 10(1): 19-40, 2006\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1ae39ff",
   "metadata": {
    "id": "a1ae39ff"
   },
   "source": [
    "## Object masks from prompts with SAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a4b25c",
   "metadata": {
    "id": "b4a4b25c"
   },
   "source": [
    "The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt. \n",
    "\n",
    "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "644532a8",
   "metadata": {
    "id": "644532a8"
   },
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07fabfee",
   "metadata": {
    "id": "07fabfee"
   },
   "source": [
    "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea65efc",
   "metadata": {
    "id": "5ea65efc"
   },
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd9a89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91dd9a89",
    "outputId": "fa11e5ff-9608-4771-d082-b05c8acfea8e"
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "    \n",
    "    !mkdir images\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg\n",
    "        \n",
    "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be845da",
   "metadata": {
    "id": "0be845da"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33681dd1",
   "metadata": {
    "id": "33681dd1"
   },
   "source": [
    "Necessary imports and helper functions for displaying points, boxes, and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b28288",
   "metadata": {
    "id": "69b28288"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc90d5",
   "metadata": {
    "id": "29bc90d5"
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06ca884f",
   "metadata": {},
   "source": [
    "# Download model checkpoint\n",
    "The checkpoint is 2.39GB, takes a few minutes for most bandwidths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7410dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "filename = \"sam_vit_h_4b8939.pth\"\n",
    "folder = \"models\"\n",
    "\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "filepath = os.path.join(folder, filename)\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    # Get the file size before downloading\n",
    "    file_size = int(urllib.request.urlopen(url).info().get(\"Content-Length\", -1))\n",
    "\n",
    "    # Start the download with progress bar\n",
    "    with tqdm(unit=\"B\", unit_scale=True, unit_divisor=1024, total=file_size, desc=filename, ncols=80) as pbar:\n",
    "        urllib.request.urlretrieve(url, filepath, reporthook=lambda b, bsize, t: pbar.update(bsize))\n",
    "else:\n",
    "    print(\"Checkpoint file already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"models/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "VkTX9wam7nhj",
   "metadata": {
    "id": "VkTX9wam7nhj"
   },
   "source": [
    "# Chest X-ray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l0RJqwoq8kHm",
   "metadata": {
    "id": "l0RJqwoq8kHm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LhXWkNR47rxA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhXWkNR47rxA",
    "outputId": "cf3e397a-c746-4f91-d183-21432580e718"
   },
   "outputs": [],
   "source": [
    "# Mount Google drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive/')\n",
    "    os.chdir('gdrive/MyDrive/ADL2022')\n",
    "except:\n",
    "    print('Google drive not mounted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are getting a download error, comment in the following lines:\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ET9ewnZ_8N7t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ET9ewnZ_8N7t",
    "outputId": "e44d152c-0475-408f-8f44-ed31d6609ad5"
   },
   "outputs": [],
   "source": [
    "# Load database with chest X-rays with lung segmentations.\n",
    "data_root='./datasets'\n",
    "data_npz='lung_field_dataset.npz'\n",
    "data_fn = os.path.join(data_root, \"lung_field_dataset.npz\")\n",
    "force_download = False\n",
    "\n",
    "if (not os.path.exists(data_fn)) or force_download:\n",
    "    download_url(\"https://sid.erda.dk/share_redirect/gCTc6o3KAh\", data_root, data_npz)\n",
    "else:\n",
    "    print('Using existing', data_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MCnFSLuY5cMZ",
   "metadata": {
    "id": "MCnFSLuY5cMZ"
   },
   "outputs": [],
   "source": [
    "def plot_image_with_segmentation(image, segmentation, ax=None):\n",
    "    \"\"\"\n",
    "    Plots an image with overlayed segmentation mask\n",
    "    \n",
    "    Returns: plt.fig and ax objects\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "    mask = np.ma.masked_where(segmentation == 0, segmentation)\n",
    "    ax.imshow(mask.squeeze(), cmap=\"Set1\", alpha=0.5)\n",
    "    return plt.gcf(), ax\n",
    "\n",
    "\n",
    "def load_npz_dataset(path, keys=('x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test')):\n",
    "    archive = np.load(path)\n",
    "    return [archive.get(key) for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_interval(image, from_min, from_max, to_min, to_max):\n",
    "    \"\"\"\n",
    "    Map values from [from_min, from_max] to [to_min, to_max]\n",
    "    \"\"\"\n",
    "    from_range = from_max - from_min\n",
    "    to_range = to_max - to_min\n",
    "    # scaled = np.array((image - from_min) / float(from_range), dtype=float)\n",
    "    scaled = (image - from_min) / float(from_range)\n",
    "    return to_min + (scaled * to_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aI_AlP5V5p8-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "aI_AlP5V5p8-",
    "outputId": "67d9db74-985e-4268-fd43-68d9f3b40ae7"
   },
   "outputs": [],
   "source": [
    "# Load train/val/test data (0 for original)\n",
    "x0_train, y0_train, x0_val, y0_val, x0_test, y0_test = load_npz_dataset(data_fn)\n",
    "\n",
    "# TODO: \n",
    "# Bring images into the correct format for SAM:\n",
    "# Image shape: (N, H, W, C=3)\n",
    "# Mask shape: (N, H, W)\n",
    "# Values: [0, 255] (uint8)\n",
    "\n",
    "def data_for_sam(x, y):\n",
    "    \"\"\"\n",
    "    Rescale images and masks for SAM\n",
    "    Makes grayscale images into RGB images /w uint8 values 0 to 255\n",
    "    \"\"\"\n",
    "    new_x = []\n",
    "    for img in x:\n",
    "        img_mapped = map_interval(img, img.min(), img.max(), 0, 255)\n",
    "        img_copied = img_mapped.repeat(3, axis=-1)\n",
    "        img_new = img_copied.astype(np.uint8)\n",
    "        new_x.append(img_new)\n",
    "        \n",
    "    return np.array(new_x), y.reshape(y.shape[0], y.shape[1], y.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52397af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = data_for_sam(x0_train, y0_train)\n",
    "x_val, y_val = data_for_sam(x0_val, y0_val)\n",
    "x_test, y_test = data_for_sam(x0_test, y0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your data should pass the following asserts\n",
    "assert x_train.shape == (112, 256, 256, 3)\n",
    "assert y_train.shape == (112, 256, 256)\n",
    "assert x_val.shape == (12, 256, 256, 3)\n",
    "assert y_val.shape == (12, 256, 256)\n",
    "assert x_test.shape == (123, 256, 256, 3)\n",
    "assert y_test.shape == (123, 256, 256)\n",
    "\n",
    "assert x_train.dtype == y_train.dtype == np.uint8\n",
    "assert np.min(x_train) == 0\n",
    "assert np.max(x_train) == 255\n",
    "example_mask = y_train[0].copy()\n",
    "example_mask[:] = 0\n",
    "# Plot an example\n",
    "fig, ax = plot_image_with_segmentation(x_train[0], example_mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9yHsXs2QugdE",
   "metadata": {
    "id": "9yHsXs2QugdE"
   },
   "source": [
    "# Single Example image\n",
    "\n",
    "Let's try to run SAM on a single example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Center of Mass (COM) for each mask, and subsequently the COM of all COMs\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "def calculate_coms_for_sides(masks):\n",
    "    left_coms = []\n",
    "    right_coms = []\n",
    "    center_split = 128  # Split masks in the middle\n",
    "    for mask in masks:\n",
    "        left_mask, right_mask = mask.copy(), mask.copy()\n",
    "        left_mask[:, center_split:] = 0 # Left side mask (x from 0 to 127)\n",
    "        right_mask[:, :center_split] = 0 # Right side mask (x from 128 to 255)\n",
    "\n",
    "        # Calculate COM for each side if there are any positive pixels\n",
    "        if np.any(left_mask):\n",
    "            left_com = center_of_mass(left_mask)\n",
    "            # Adjust x-coordinate for left side to fit the original mask coordinates\n",
    "            left_com = (left_com[0], left_com[1])\n",
    "            left_coms.append(left_com)\n",
    "        \n",
    "        if np.any(right_mask):\n",
    "            right_com = center_of_mass(right_mask)\n",
    "            # Adjust x-coordinate for right side to fit the original mask coordinates\n",
    "            right_com = (right_com[0], right_com[1])\n",
    "            right_coms.append(right_com)\n",
    "    \n",
    "    return np.array(left_coms), np.array(right_coms)\n",
    "\n",
    "def calculate_mean_com(coms):\n",
    "    \"\"\"Returns mean as an integer\"\"\"\n",
    "    mean = np.mean(coms, axis=0)\n",
    "    rounded = np.round(mean).astype(int)\n",
    "    xy_correction = np.array([rounded[1], rounded[0]])\n",
    "    return xy_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_lung_coms, right_lung_coms = calculate_coms_for_sides(y_train)\n",
    "left_lung_mean_com = calculate_mean_com(left_lung_coms)\n",
    "right_lung_mean_com = calculate_mean_com(right_lung_coms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_lung_coms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = np.array([left_lung_mean_com, right_lung_mean_com, [128, 128]])\n",
    "input_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m6tcM2rQs_Hj",
   "metadata": {
    "id": "m6tcM2rQs_Hj"
   },
   "outputs": [],
   "source": [
    "example_img, example_mask = x_train[0], y_train[0]\n",
    "\n",
    "input_points = np.array([left_lung_mean_com, right_lung_mean_com, [128, 128], [128, 240], [15, 50], [256-15, 50]]) # Picked left and right lung COM and center of image\n",
    "input_label = np.array([1, 1, 0, 0, 0, 0]) # Get labels for the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BhM_AohdukYD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "id": "BhM_AohdukYD",
    "outputId": "60ba6cc0-1675-4266-9b2e-a893039509ac"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(example_img)\n",
    "show_points(input_points, input_label, plt.gca()) # You may also use other prompt methods!\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PS-UajooqKDN",
   "metadata": {
    "id": "PS-UajooqKDN"
   },
   "outputs": [],
   "source": [
    "predictor.set_image(example_img)\n",
    "\n",
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_points,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iE0pIkugqV-i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iE0pIkugqV-i",
    "outputId": "94e9aef4-e541-4017-a3f8-075a8c6c7023"
   },
   "outputs": [],
   "source": [
    "# Note that the \"score\" here is an estimation of the mask quality, not the quality of the segmentation compared to the ground truth.\n",
    "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(example_img.squeeze())\n",
    "    show_mask(mask, plt.gca())\n",
    "    show_points(input_points, input_label, plt.gca())\n",
    "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e2bf35b",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "f1 = torchmetrics.F1Score(task=\"binary\")\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for img, mask_gt in zip(x_val, y_val):\n",
    "    predictor.set_image(img)\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_points,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    mask_pred = masks[np.argmax(scores)]\n",
    "    f1.update(mask_pred, mask_gt)\n",
    "    f1_scores.append(f1.compute())\n",
    "\n",
    "f1_scores = np.array(f1_scores)\n",
    "mean_f1 = f1_scores.mean() # TODO: Compute mean F1 score\n",
    "std_f1 = f1_scores.std() # TODO: Compute standard deviation of F1 scores\n",
    "\n",
    "print(f\"Mean F1 score: {mean_f1:.4f}\")\n",
    "print(f\"Standard deviation: {std_f1:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b6b9a84",
   "metadata": {},
   "source": [
    "# Using Bounding Boxes from GT segmentations as Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25701a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding boxes from segmentation masks\n",
    "# bonding box format [x0, y0, x1, y1]\n",
    "\n",
    "# TODO: Implement bounding box extraction from segmentation masks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e28ad18",
   "metadata": {},
   "source": [
    "# Object Detection Model to predict Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement an object detection model to find the left and right lung bounding boxes"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
